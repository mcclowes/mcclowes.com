---
title: "AI is not a hammer"
authors: mcclowes
tags: [ai, technology, society]
---

We seem to have stumbled into a strange contradiction: we demand perfection from our machines, while tolerating imperfection from ourselves. The same people who chuckle at human error in the workplace will denounce AI systems for the slightest misstep. "It hallucinated a fact!" Yes. And you never have?

<!--truncate-->

This aversion to error-prone AI is understandable but ultimately misguided. We hold AI to a standard that no human could meet, and then declare it untrustworthy when it falls short. But the problem isn't that AI is fallible. The problem is that we expect it not to be.

Human systems function not because individuals are flawless, but because we build resilient structures around fallibility. We double-check, we cross-reference, we ask a friend, we add context, we create escalation paths and review processes. We don't trust humans blindly; we build around them.

So why do we not build around AI in the same way?

The answer is cultural. We still think of tools as passive. A hammer doesn't need oversight. A spreadsheet doesn't hallucinate. But AI isn't that kind of tool. It's an active participant, a co-pilot, a suggestive partner. And like any partner, it needs checks, context, and backup. It needs systems.

This doesn't mean letting AI off the hook. As we rely on it more, the stakes rise, and our expectations should too. But our expectation should be excellence, not perfection. What we need is a shift in mindset: from demanding infallibility to designing for resilience.

We should be building human-AI collaborations that are robust to failureâ€”where the strengths of one offset the weaknesses of the other. This might look like tools that flag AI uncertainty, workflows that encourage human review, or processes that treat AI outputs as suggestions rather than truths. It's not that different from how we already work with people.

The future of AI isn't about replacing flawed humans with flawless machines. It's about embracing the fact that both are imperfect, and building systems where those imperfections matter less.

If we get that right, we don't need perfect AI. We need good enough AI, embedded in great systems. 