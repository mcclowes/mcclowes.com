---
title: "We don't understand human intelligence"
authors: mcclowes
tags: [ai]
draft: true
---

We're building artificial intelligence without a clear definition of the natural kind. This seems like a problem.

<!--truncate-->

## The measurement gap

We measure AI against benchmarks: tests, tasks, comparisons to human performance. But we don't have a coherent theory of what human intelligence actually is.

IQ tests measure something. But intelligence researchers still debate what. General factor? Multiple intelligences? Cultural artefact?

## Building toward an undefined target

When we say we want "human-level AI" or "artificial general intelligence," what do we mean?

- Passing tests humans pass?
- Doing jobs humans do?
- Reasoning like humans reason?
- Something else entirely?

Without clarity on the target, how do we know if we're getting closer?

## The practical problem

This isn't just philosophical. It affects how we:
- Evaluate AI capabilities
- Predict what AI will be able to do
- Design AI systems
- Regulate AI development

If we don't understand human intelligence, we're navigating by landmarks we can't see clearly.

## Maybe it doesn't matter

One response: who cares what intelligence is? We can build useful AI without defining it precisely.

Maybe. But the ambiguity creates confusion. Every claim about AI being "intelligent" or "not really intelligent" is made against an undefined standard.

We might not need a complete theory. But we could use a clearer one.
